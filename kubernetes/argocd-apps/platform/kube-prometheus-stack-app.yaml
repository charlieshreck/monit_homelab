---
apiVersion: argoproj.io/v1alpha1
kind: Application
metadata:
  name: kube-prometheus-stack
  namespace: argocd
  annotations:
    argocd.argoproj.io/sync-wave: "3"
spec:
  project: monitoring
  source:
    repoURL: https://prometheus-community.github.io/helm-charts
    chart: kube-prometheus-stack
    targetRevision: 69.3.1
    helm:
      valuesObject:
        # Prometheus configuration
        prometheus:
          prometheusSpec:
            retention: 30d
            retentionSize: "45GB"
            storageSpec:
              volumeClaimTemplate:
                spec:
                  accessModes: ["ReadWriteOnce"]
                  resources:
                    requests:
                      storage: 50Gi
            # Fix storage permissions for local-path provisioner
            securityContext:
              runAsUser: 1000
              runAsGroup: 2000
              fsGroup: 2000
              runAsNonRoot: true
            # Init container to fix volume permissions
            initContainers:
              - name: init-chown-data
                image: busybox:latest
                command: ['sh', '-c', 'chown -R 1000:2000 /prometheus && chmod -R 775 /prometheus']
                volumeMounts:
                  - name: prometheus-kube-prometheus-stack-prometheus-db
                    mountPath: /prometheus
                    subPath: prometheus-db
                securityContext:
                  runAsUser: 0
                  runAsNonRoot: false
            # Allow writes to mounted volumes
            containers:
              - name: prometheus
                securityContext:
                  readOnlyRootFilesystem: false
                  allowPrivilegeEscalation: false
                  capabilities:
                    drop:
                      - ALL
            resources:
              requests:
                cpu: 500m
                memory: 2Gi
              limits:
                cpu: 2000m
                memory: 4Gi
            # Mount production cluster credentials
            secrets:
              - production-cluster-credentials
            # Enable remote write to VictoriaMetrics
            remoteWrite:
              - url: http://victoria-metrics-victoria-metrics-single-server:8428/api/v1/write
                queueConfig:
                  capacity: 10000
                  maxShards: 30
                  minShards: 1
                  maxSamplesPerSend: 5000
                  batchSendDeadline: 5s
                  minBackoff: 30ms
                  maxBackoff: 100ms
            # Scrape configs for monitoring targets
            additionalScrapeConfigs:
              # NOTE: proxmox-ruapehu scrape DISABLED - target is down.
              # Proxmox health covered by Gatus + infrastructure-mcp.
              # Restore with proper API token auth via Infisical when needed.
              # - job_name: 'proxmox-ruapehu'
              #   scheme: https
              #   tls_config:
              #     insecure_skip_verify: true
              #   basic_auth:
              #     username: root@pam
              #     password: H4ckwh1z
              #   static_configs:
              #     - targets: ['10.10.0.10:8006']
              #   metrics_path: '/api2/prometheus'
              #   params:
              #     format: ['prometheus']
              # NOTE: proxmox-carrick REMOVED - host decommissioned

              # Production Talos Cluster - Kubelet Metrics (via API server proxy)
              - job_name: 'talos-kubelets'
                scheme: https
                tls_config:
                  ca_file: /etc/prometheus/secrets/production-cluster-credentials/ca.crt
                  insecure_skip_verify: false
                bearer_token_file: /etc/prometheus/secrets/production-cluster-credentials/token
                kubernetes_sd_configs:
                  - role: node
                    api_server: 'https://10.10.0.40:6443'
                    tls_config:
                      ca_file: /etc/prometheus/secrets/production-cluster-credentials/ca.crt
                    bearer_token_file: /etc/prometheus/secrets/production-cluster-credentials/token
                relabel_configs:
                  - action: labelmap
                    regex: __meta_kubernetes_node_label_(.+)
                  - target_label: __address__
                    replacement: 10.10.0.40:6443
                  - source_labels: [__meta_kubernetes_node_name]
                    regex: (.+)
                    target_label: __metrics_path__
                    replacement: /api/v1/nodes/${1}/proxy/metrics
                  - target_label: cluster
                    replacement: production

              # Production Talos Cluster - cAdvisor Metrics (via API server proxy)
              - job_name: 'talos-cadvisor'
                scheme: https
                tls_config:
                  ca_file: /etc/prometheus/secrets/production-cluster-credentials/ca.crt
                  insecure_skip_verify: false
                bearer_token_file: /etc/prometheus/secrets/production-cluster-credentials/token
                kubernetes_sd_configs:
                  - role: node
                    api_server: 'https://10.10.0.40:6443'
                    tls_config:
                      ca_file: /etc/prometheus/secrets/production-cluster-credentials/ca.crt
                    bearer_token_file: /etc/prometheus/secrets/production-cluster-credentials/token
                relabel_configs:
                  - action: labelmap
                    regex: __meta_kubernetes_node_label_(.+)
                  - target_label: __address__
                    replacement: 10.10.0.40:6443
                  - source_labels: [__meta_kubernetes_node_name]
                    regex: (.+)
                    target_label: __metrics_path__
                    replacement: /api/v1/nodes/${1}/proxy/metrics/cadvisor
                  - target_label: cluster
                    replacement: production

              # Production Talos Cluster - API Server
              - job_name: 'talos-apiserver'
                scheme: https
                tls_config:
                  ca_file: /etc/prometheus/secrets/production-cluster-credentials/ca.crt
                  insecure_skip_verify: false
                bearer_token_file: /etc/prometheus/secrets/production-cluster-credentials/token
                static_configs:
                  - targets: ['10.10.0.40:6443']
                    labels:
                      cluster: 'production'
                metrics_path: '/metrics'

              # Talos monitoring cluster - kubelet metrics
              - job_name: 'talos-monitor'
                scheme: https
                tls_config:
                  insecure_skip_verify: true
                bearer_token_file: /var/run/secrets/kubernetes.io/serviceaccount/token
                static_configs:
                  - targets: ['10.30.0.20:10250']
                    labels:
                      cluster: 'monitoring'
                metrics_path: '/metrics'

              # NOTE: claude-agent and claude-validator scrapes REMOVED.
              # Services are not deployed. Re-add when Phase 4 services deploy.

              # Production Talos Cluster - kube-state-metrics (via NodePort)
              - job_name: 'talos-kube-state-metrics'
                scheme: http
                static_configs:
                  - targets: ['10.10.0.40:30081']  # NodePort on any production node
                    labels:
                      cluster: 'production'
                      job: 'kube-state-metrics'

              # Production Talos Cluster - node-exporter (direct to node IPs)
              - job_name: 'talos-node-exporter'
                scheme: http
                static_configs:
                  - targets:
                      - '10.10.0.40:9100'  # talos-cp-01
                      - '10.10.0.41:9100'  # talos-worker-01
                      - '10.10.0.42:9100'  # talos-worker-02
                      - '10.10.0.43:9100'  # talos-worker-03
                    labels:
                      cluster: 'production'
                      job: 'node-exporter'

        # Grafana configuration
        grafana:
          enabled: true
          adminPassword: H4ckwh1z
          # Disable default datasource to avoid conflicts
          defaultDatasourceEnabled: false
          # Standalone image renderer service (deployed separately)
          env:
            GF_RENDERING_SERVER_URL: http://grafana-image-renderer.monitoring:8081/render
            GF_RENDERING_CALLBACK_URL: http://kube-prometheus-stack-grafana.monitoring:80/
            GF_LOG_FILTERS: rendering:debug
          persistence:
            enabled: true
            size: 10Gi
          # Fix storage permissions
          securityContext:
            runAsUser: 472
            runAsGroup: 472
            fsGroup: 472
          resources:
            requests:
              cpu: 100m
              memory: 256Mi
            limits:
              cpu: 500m
              memory: 512Mi
          datasources:
            datasources.yaml:
              apiVersion: 1
              datasources:
                - name: Prometheus
                  type: prometheus
                  url: http://kube-prometheus-stack-prometheus:9090
                  isDefault: false
                  access: proxy
                  uid: prometheus
                  jsonData:
                    httpMethod: POST
                    timeInterval: 30s
                - name: VictoriaMetrics
                  type: prometheus
                  url: http://victoria-metrics-victoria-metrics-single-server:8428
                  access: proxy
                  uid: victoriametrics
                  isDefault: false
                  jsonData:
                    httpMethod: POST
                    timeInterval: 30s
                - name: VictoriaLogs
                  type: loki
                  url: http://victoria-logs-victoria-logs-single-server:9428/select/logsql
                  access: proxy
                  uid: victorialogs
                  isDefault: false
                  jsonData:
                    maxLines: 1000
          dashboardProviders:
            dashboardproviders.yaml:
              apiVersion: 1
              providers:
                - name: 'default'
                  orgId: 1
                  folder: ''
                  type: file
                  disableDeletion: false
                  editable: true
                  options:
                    path: /var/lib/grafana/dashboards/default
          dashboards:
            default:
              # Kubernetes Cluster Monitoring
              kubernetes-cluster:
                gnetId: 7249
                revision: 1
                datasource: Prometheus
              kubernetes-views-global:
                gnetId: 15757
                revision: 37
                datasource: Prometheus
              kubernetes-views-namespaces:
                gnetId: 15758
                revision: 35
                datasource: Prometheus
              kubernetes-views-pods:
                gnetId: 15759
                revision: 28
                datasource: Prometheus
              # Node Exporter
              node-exporter-full:
                gnetId: 1860
                revision: 37
                datasource: Prometheus
              # VictoriaMetrics
              victoriametrics:
                gnetId: 10229
                revision: 2
                datasource: VictoriaMetrics
              victoriametrics-cluster:
                gnetId: 11176
                revision: 18
                datasource: VictoriaMetrics
              # Proxmox
              proxmox:
                gnetId: 10347
                revision: 5
                datasource: Prometheus

        # AlertManager configuration
        alertmanager:
          enabled: true
          # Configure alert routing to agentic cluster alerting-pipeline
          config:
            global:
              resolve_timeout: 5m
            route:
              receiver: 'alerting-pipeline'
              group_by: ['namespace', 'alertname']
              group_wait: 30s
              group_interval: 5m
              repeat_interval: 4h
              routes:
                # Silence watchdog alerts
                - match:
                    alertname: 'Watchdog'
                  receiver: 'null'
                # Info alerts go to null
                - match:
                    alertname: 'InfoInhibitor'
                  receiver: 'null'
                # Renovate CronJob leaves stuck pods - expected
                - match:
                    alertname: 'KubeJobNotCompleted'
                    namespace: 'renovate'
                  receiver: 'null'
                # Container waiting during rollouts is transient
                - match:
                    alertname: 'KubeContainerWaiting'
                  receiver: 'null'
                # Critical alerts - immediate
                - match:
                    severity: critical
                  receiver: 'alerting-pipeline'
                  group_wait: 10s
                  repeat_interval: 2h
                # Warning alerts
                - match:
                    severity: warning
                  receiver: 'alerting-pipeline'
                  repeat_interval: 12h
            receivers:
              - name: 'null'
              - name: 'alerting-pipeline'
                webhook_configs:
                  # Keep - central alert aggregation hub
                  # Keep forwards to LangGraph after dedup/consolidation
                  - url: 'http://10.20.0.40:31105/alerts/event/prometheus'
                    send_resolved: true
                    http_config:
                      follow_redirects: true
                      basic_auth:
                        username: api_key
                        password: 16b7de2a-5d6d-4021-a774-53a083edc28e
            inhibit_rules:
              - source_matchers:
                  - severity = critical
                target_matchers:
                  - severity =~ warning|info
                equal: ['namespace', 'alertname']
              - source_matchers:
                  - severity = warning
                target_matchers:
                  - severity = info
                equal: ['namespace', 'alertname']
          alertmanagerSpec:
            storage:
              volumeClaimTemplate:
                spec:
                  accessModes: ["ReadWriteOnce"]
                  resources:
                    requests:
                      storage: 10Gi
            resources:
              requests:
                cpu: 50m
                memory: 128Mi
              limits:
                cpu: 200m
                memory: 256Mi

        # Node exporter
        nodeExporter:
          enabled: true

        # Kube-state-metrics
        kubeStateMetrics:
          enabled: true

        # DISABLED: Talos control plane not exposed as pods
        # These ServiceMonitors cause TargetDown alerts
        kubeScheduler:
          enabled: false
        kubeControllerManager:
          enabled: false
        kubeProxy:
          enabled: false

        # Default rules
        # NOTE: Disabled Talos-incompatible rules (control plane not exposed as pods)
        defaultRules:
          create: true
          rules:
            alertmanager: true
            etcd: false           # etcd not exposed in Talos
            kubeApiserver: true
            kubeScheduler: false  # DISABLED: Talos control plane not exposed
            kubeSchedulerAlerting: false
            kubeSchedulerRecording: false
            kubeControllerManager: false  # DISABLED: Talos control plane not exposed
            kubelet: true
            kubeProxy: false      # DISABLED: Talos uses Cilium, not kube-proxy
            kubePrometheusNodeRecording: true
            node: true
            prometheus: true
            general: true
            k8s: true

  destination:
    server: https://10.30.0.20:6443
    namespace: monitoring

  syncPolicy:
    automated:
      prune: true
      selfHeal: true
    syncOptions:
      - CreateNamespace=true
      - ServerSideApply=true
